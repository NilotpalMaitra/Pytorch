{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDyxIkirswJx6lO6oQAD9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilotpalMaitra/Pytorch/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BfVmdyEs9KI",
        "outputId": "87d46d0c-48e9-4a53-aeef-88f82006b86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manipulation through the operations."
      ],
      "metadata": {
        "id": "-juGv4ciwRFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor from a list\n",
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "print(\"Tensor from list:\", tensor1)\n",
        "\n",
        "# Create a tensor of zeros with shape (2, 3)\n",
        "tensor2 = torch.zeros(2, 3)\n",
        "print(\"Tensor of zeros:\", tensor2)\n",
        "\n",
        "\n",
        "# Create a random tensor with shape (3, 2)\n",
        "tensor3 = torch.rand(3, 2)\n",
        "print(\"Random tensor:\", tensor3)\n",
        "\n",
        "# Performing operations on Tensors\n",
        "\n",
        "# Addition\n",
        "result_add = tensor1 + tensor2\n",
        "print(\"Addition result:\", result_add)\n",
        "\n",
        "\n",
        "# Multiplication\n",
        "result_mul = tensor2 * 5\n",
        "print(\"Multiplication result:\", result_mul)\n",
        "\n",
        "\n",
        "# Matrix multiplication\n",
        "result_matmul = torch.matmul(tensor2, tensor3)\n",
        "print(\"Matrix multiplication result:\", result_matmul)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO5pTeb6v2kQ",
        "outputId": "c73b9e6d-6c30-4656-c807-b428c22d840b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list: tensor([1, 2, 3])\n",
            "Tensor of zeros: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Random tensor: tensor([[0.0462, 0.2631],\n",
            "        [0.3259, 0.4814],\n",
            "        [0.6692, 0.2332]])\n",
            "Addition result: tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.]])\n",
            "Multiplication result: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Matrix multiplication result: tensor([[0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tensors with requires_grad=True to track computation history\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# Perform a computation\n",
        "z = x**2 + y**3\n",
        "print(\"Output tensor z:\", z)\n",
        "\n",
        "# Compute gradients\n",
        "z.backward()\n",
        "print(\"Gradient of x:\", x.grad)\n",
        "print(\"Gradient of y:\", y.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj0S7ShtwaHp",
        "outputId": "332fab62-18df-43c1-dd8f-1b643e1993a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor z: tensor(31., grad_fn=<AddBackward0>)\n",
            "Gradient of x: tensor(4.)\n",
            "Gradient of y: tensor(27.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network using pytorch\n",
        "using nn.module and nn.Parameter\n"
      ],
      "metadata": {
        "id": "BGD7XmWGttVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "\tdef __init__(self, input_size, hidden_size, output_size):\n",
        "\t\tsuper(SimpleNN, self).__init__()\n",
        "\t\tself.fc1 = nn.Linear(input_size, hidden_size) # Input layer\n",
        "\t\tself.relu = nn.ReLU()\t\t\t\t\t\t # Activation function\n",
        "\t\tself.fc2 = nn.Linear(hidden_size, output_size) # Output layer\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.fc1(x)\n",
        "\t\tx = self.relu(x)\n",
        "\t\tx = self.fc2(x)\n",
        "\t\treturn x\n"
      ],
      "metadata": {
        "id": "Lj6oFWJBtdg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using random seed for reproducibility, define the input, hidden, and output sizes of the nerual network architecture. Defining the loss function (CrossEntropyLoss) and optimizer (Adam), converting hte training data to PyTorch tensors, and train the model for a fixed number of epochs."
      ],
      "metadata": {
        "id": "XDDZDEgauszw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Define the input size, hidden size, and output size of the neural network\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 10\n",
        "output_size = len(iris.target_names)\n",
        "\n",
        "\n",
        "# Instantiate the neural network\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# Convert datto PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "\t# Forward pass\n",
        "\toutputs = model(X_train_tensor)\n",
        "\tloss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "\t# Backward pass and optimization\n",
        "\toptimizer.zero_grad()\n",
        "\tloss.backward()\n",
        "\toptimizer.step()\n",
        "\n",
        "\t# Print the loss every 10 epochs\n",
        "\tif (epoch+1) % 10 == 0:\n",
        "\t\tprint(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEttn5tFuIi-",
        "outputId": "e2735a9e-bf70-4930-f0cc-a63438e47213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.7783\n",
            "Epoch [20/100], Loss: 0.5399\n",
            "Epoch [30/100], Loss: 0.3921\n",
            "Epoch [40/100], Loss: 0.2934\n",
            "Epoch [50/100], Loss: 0.2166\n",
            "Epoch [60/100], Loss: 0.1639\n",
            "Epoch [70/100], Loss: 0.1284\n",
            "Epoch [80/100], Loss: 0.1050\n",
            "Epoch [90/100], Loss: 0.0902\n",
            "Epoch [100/100], Loss: 0.0800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the trained model (Converting the test dataset from NumPy arrays into the PyTorch Sensors using the torch.FloatTensor()and torch.LongTensor())"
      ],
      "metadata": {
        "id": "OkaHlNH_we4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "with torch.no_grad():\n",
        "\tX_test_tensor = torch.FloatTensor(X_test)\n",
        "\ty_test_tensor = torch.LongTensor(y_test)\n",
        "\toutputs = model(X_test_tensor)\n",
        "\t_, predicted = torch.max(outputs, 1)\n",
        "\taccuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "\tprint(f'Accuracy on the test set: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn_0RFkYwcs8",
        "outputId": "6dd63aa5-669e-4df1-b91b-9f3eb4152e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working with Data in PyTorch\n",
        "- Loading : Using Dataloader and Dataset\n",
        "using 'len' and 'getitem' methods to create Custom dataset for model building using Pytorch"
      ],
      "metadata": {
        "id": "6BDlrhrlxmFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "\tdef __init__(self, data, targets):\n",
        "\t\tself.data = data\n",
        "\t\tself.targets = targets\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.data[idx], self.targets[idx]\n",
        "\n",
        "\n",
        "# Example data\n",
        "data = torch.randn(100, 3, 32, 32) # Example image data\n",
        "targets = torch.randint(0, 10, (100,)) # Example target labels\n",
        "\n",
        "\n",
        "# Create custom dataset\n",
        "custom_dataset = CustomDataset(data, targets)\n",
        "\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "num_workers = 4\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size,\n",
        "\t\t\t\t\t\tshuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "\n",
        "# Iterate over batches\n",
        "for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "\tprint(\n",
        "\t\tf\"Batch {batch_idx+1}: Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRjXa9i1wiPt",
        "outputId": "66c379e8-2835-4748-e7bc-f3ae58ac825f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 2: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 3: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 4: Inputs shape: torch.Size([4, 3, 32, 32]), Targets shape: torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Data: Transformations and Normalization"
      ],
      "metadata": {
        "id": "hdo2WJ2yx4BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "\ttransforms.Resize(256),\t\t\t # Resize images to 256x256\n",
        "\ttransforms.RandomCrop(224),\t\t # Randomly crop images to 224x224\n",
        "\ttransforms.RandomHorizontalFlip(), # Randomly flip images horizontally\n",
        "\ttransforms.ToTensor(),\t\t\t # Convert images to PyTorch tensors\n",
        "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
        "\t\t\t\t\t\t0.229, 0.224, 0.225]) # Normalize images\n",
        "])\n",
        "\n",
        "# Example of applying transformations to image\n",
        "example_image = transforms.ToPILImage()(\n",
        "\ttorch.randn(3, 256, 256)) # Example image tensor\n",
        "transformed_image = transform(example_image)\n",
        "\n",
        "\n",
        "print(\"Transformed image shape:\", transformed_image.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsMd3cizx9GR",
        "outputId": "e81f162a-2eed-428e-ec2c-493b45fe20a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed image shape: torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Custom datasets\n",
        "- creating a dataset of a specific structure and format\n",
        "- '__len__' method returns the total number of samples in the dataset\n",
        "-'__getitem__' method fetches the sample and its corresponding target."
      ],
      "metadata": {
        "id": "J8H9gJzdyA7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# Define custom dataset class by subclassing torch.utils.data.Dataset\n",
        "class CustomDataset(Dataset):\n",
        "\tdef __init__(self, data, targets):\n",
        "\t\tself.data = data\n",
        "\t\tself.targets = targets\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t# Return the total number of samples in the dataset\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\t# Retrieve and return a sample and its corresponding target based on the given index\n",
        "\t\tsample = self.data[index]\n",
        "\t\ttarget = self.targets[index]\n",
        "\t\treturn sample, target\n",
        "\n",
        "\n",
        "# Example data and targets\n",
        "data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "targets = torch.tensor([0, 1, 0, 1])\n",
        "\n",
        "# Create instance of the custom dataset\n",
        "custom_dataset = CustomDataset(data, targets)\n",
        "\n",
        "# Create a data loader to iterate over the dataset in batches\n",
        "batch_size = 2\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over the data loader to access batches of data\n",
        "for batch_idx, (samples, targets) in enumerate(data_loader):\n",
        "\tprint(f\"Batch {batch_idx}:\")\n",
        "\tprint(\"Samples:\", samples)\n",
        "\tprint(\"Targets:\", targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Oe8BNoyAXg",
        "outputId": "35e6f092-cb23-4984-dd44-6e1b2182b530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "Samples: tensor([[5, 6],\n",
            "        [3, 4]])\n",
            "Targets: tensor([0, 1])\n",
            "Batch 1:\n",
            "Samples: tensor([[1, 2],\n",
            "        [7, 8]])\n",
            "Targets: tensor([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers\n",
        "- Stoichastic Gradient Descent(SGD) = updates parameters in the direction opposite to the gradient of the loss function with respect to the parameters\n",
        "- Adam = based on the adaptive learning rate optimization that computes adaptive learningn rates for each parameter. It combines the advantages of AdaGrad and RMSProp(Root Mean Square Propogation)\n",
        "- Adagrad = This algorithm adapts the learning rate of each parameter based on the historical gradients."
      ],
      "metadata": {
        "id": "9mV_PcLuzAwG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OANjH1Rdyayk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}